{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline & Data Warehousing (DWH)\n",
    "    Compiled by: Alem Fitwi \n",
    "    Binghamton, New York\n",
    "    September 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Data Pipeline (DPL)?\n",
    "- A mechanism/Scheme to transfer data from source (point of creation) to the the point of transformation, and then consumption/sink.\n",
    "- It is just like water fetching in the old days or like water pipes in the modern days that bring waters to our homes for consumption.\n",
    "\n",
    "- DPL is a mechanism to transfer data from point A to point Z\n",
    "\n",
    "   $$A\\ \\rightarrow\\ B\\ \\rightarrow\\ C\\ \\rightarrow\\ D\\ ...\\ \\rightarrow\\ Z$$\n",
    "   \n",
    "           A  --> Data Producer\n",
    "           B, C, .. --> Data Pipeline\n",
    "           Z --> Data COnsumer/Sink\n",
    "- These are the pints where the following transformations take place\n",
    "    - Data Cleansing\n",
    "    - Data Governance\n",
    "    - Data Enrichment\n",
    "    - Data Processing\n",
    "<img src = './figs/dpl1.png'>\n",
    "             \n",
    "                         Fig 1.1 Water fetching/Water Pipelines\n",
    "                         \n",
    "- Analogy:\n",
    "    - Data Source --> Water Source\n",
    "    - Data Pipeline --> Water-pipes\n",
    "    - Sinks --> Consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Types of Data Pipeline\n",
    "1. **Batch**: Batch processing refers to processing of high volume of data in batch within a specific time span. \n",
    "    - Batch Data Pipeline\n",
    "2. **Streaming**: Stream processing refers to processing of continuous stream of data immediately as it is produced.\n",
    "   - Real Time Data Pipeline\n",
    "    - Uses Real tiem message breaking services\n",
    "3. **Lambda Architecture**: Real Time + Batch\n",
    "    - The Best of both worlds\n",
    "    \n",
    "                            --------------->  Batch Layer----> Service Layer    <------>\n",
    "                           |                  Master Data      Batch View 1, 2, 3       |\n",
    "                           |                                                            |\n",
    "                        Data Source                                                   Query\n",
    "                           |                                                             |\n",
    "                           |                 Speed Layer                                 |\n",
    "                           |                 Real-time View                              |\n",
    "                            ---------------> From T0 to T1 ... <------------------------>\n",
    "\n",
    "\n",
    "                            \n",
    " <img src = './figs/dpl2.png'>\n",
    " \n",
    "                                 Fig 1.2 Common Data Pipeline Look\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ETL - Extract, Transform, & Load**: Trandition Data Processing Model\n",
    "- **Apache KAFKA**: Streaming Messages Service\n",
    "    - An open-source stream-processing platform for handling real-time data feeds.\n",
    "    \n",
    "- **Master Database Management (MDM)**, helps create one single master reference source for all critical business data, leading to fewer errors and less redundancy in business processes. \n",
    "- **Data Lake**: Data lake is ideal for the users who indulge in deep analysis. Such users include data scientists who need advanced analytical tools with capabilities such as predictive modeling and statistical analysis. \n",
    "    - A Data Lake is a storage repository that can store a large amount of structured, semi-structured, and unstructured data. It is a place to store every type of data in its native format with no fixed limits on account size or file. It offers a large amount of data quantity for increased analytical performance and native integration.\n",
    "    - Data Lake is like a large container which is very similar to real lake and rivers. Just like in a lake, you have multiple tributaries coming in; similarly, a data lake has structured data, unstructured data, machine to machine, logs flowing through in real-time.\n",
    "    - A Data Lake is a large size storage repository that holds a large amount of raw data in its original format until the time it is needed. Every data element in a Data lake is given a unique identifier and tagged with a set of extended metadata tags. It offers wide varieties of analytic capabilities.\n",
    "- **Data Warehouse**: The data warehouse is ideal for operational users because of being well structured, easy to use and understand.\n",
    "    - Data Warehouse stores data in files or folders which helps to organize and use the data to take strategic decisions. This storage system also gives a multi-dimensional view of atomic and summary data. The important functions which are needed to perform are:\n",
    "        - Data Extraction\n",
    "        - Data Cleaning\n",
    "        - Data Transformation\n",
    "        - Data Loading and Refreshing\n",
    "- KEY DIFFERENCE\n",
    "    - Data Lake stores all data irrespective of the source and its structure whereas Data Warehouse stores data in quantitative metrics with their attributes.\n",
    "    - Data Lake is a storage repository that stores huge structured, semi-structured and unstructured data while Data Warehouse is blending of technologies and component which allows the strategic use of data.\n",
    "    - Data Lake defines the schema after data is stored whereas Data Warehouse defines the schema before data is stored.\n",
    "    - Data Lake uses the ELT(Extract Load Transform) process while the Data Warehouse uses ETL(Extract Transform Load) process.\n",
    "    - Comparing Data lake vs Warehouse, Data Lake is ideal for those who want in-depth analysis whereas Data Warehouse is ideal for operational users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are key differences between data lakes vs data warehouse:\n",
    "\n",
    "#### Parameters: Storage\n",
    "- Data Lake: In the data lake, all data is kept irrespective of the source and its structure. Data is kept in its raw form. It is only transformed when it is ready to be used.\n",
    "- Data Warehouse: A data warehouse will consist of data that is extracted from transactional systems or data which consists of quantitative metrics with their attributes. The data is cleaned and transformed\n",
    "\n",
    "#### Parameters:  History\t\n",
    "- Big data technologies used in data lakes is relatively new.\t\n",
    "- Data warehouse concept, unlike big data, had been used for decades.\n",
    "\n",
    "#### Data Capturing\t\n",
    "- Captures all kinds of data and structures, semi-structured and unstructured in their original form from source systems.\t\n",
    "- Captures structured information and organizes them in schemas as defined for data warehouse purposes\n",
    "\n",
    "#### Data Timeline\t\n",
    "- Data lakes can retain all data. This includes not only the data that is in use but also data that it might use in the future. Also, data is kept for all time, to go back in time and do an analysis.\t\n",
    "- In the data warehouse development process, significant time is spent on analyzing various data sources.\n",
    "\n",
    "#### Users\t\n",
    "- Data lake is ideal for the users who indulge in deep analysis. Such users include data scientists who need advanced analytical tools with capabilities such as predictive modeling and statistical analysis.\t\n",
    "- The data warehouse is ideal for operational users because of being well structured, easy to use and understand.\n",
    "\n",
    "#### Storage Costs\t\n",
    "- Data storing in big data technologies are relatively inexpensive then storing data in a data warehouse.\t\n",
    "- Storing data in Data warehouse is costlier and time-consuming.\n",
    "\n",
    "#### Task\t\n",
    "- Data lakes can contain all data and data types; it empowers users to access data prior the process of transformed, cleansed and structured.\t\n",
    "- Data warehouses can provide insights into pre-defined questions for pre-defined data types.\n",
    "\n",
    "#### Processing time\t\n",
    "- Data lakes empower users to access data before it has been transformed, cleansed and structured. Thus, it allows users to get to their result more quickly compares to the traditional data warehouse.\t\n",
    "- Data warehouses offer insights into pre-defined questions for pre-defined data types. So, any changes to the data warehouse needed more time.\n",
    "\n",
    "#### Position of Schema\t\n",
    "- Typically, the schema is defined after data is stored. This offers high agility and ease of data capture but requires work at the end of the process\t\n",
    "- Typically schema is defined before data is stored. Requires work at the start of the process, but offers performance, security, and integration.\n",
    "\n",
    "#### Data processing\t\n",
    "- Data Lakes use of the ELT (Extract Load Transform) process.\t\n",
    "- Data warehouse uses a traditional ETL (Extract Transform Load) process.\n",
    "\n",
    "#### Complain\t\n",
    "- Data is kept in its raw form. It is only transformed when it is ready to be used.\t\n",
    "- The chief complaint against data warehouses is the inability, or the problem faced when trying to make change in in them.\n",
    "\n",
    "#### Key Benefits\t\n",
    "- They integrate different types of data to come up with entirely new questions as these users not likely to use data warehouses because they may need to go beyond its capabilities\n",
    "- Most users in an organization are operational. These type of users only care about reports and key performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pipeline Designing\n",
    "### Generic:\n",
    "<img src = './figs/dpl3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Example:\n",
    "\n",
    "<img src = './figs/dpl4.png'>\n",
    "\n",
    "- **Google Cloud Data Fusion**:  , No need of programming (High-level)\n",
    "    - Fully managed, cloud-native data integration at any scale.\n",
    "    - New customers get $300 in free credits to spend on Google Cloud during the first 90 days. All customers get the first 120 hours of pipeline development per month, per account, free of charge.\n",
    "        - Visual point-and-click interface enabling code-free deployment of ETL/ELT data pipelines\n",
    "\n",
    "        - Broad library of 150+ pre-configured connectors and transformations, at no additional cost\n",
    "        - Natively integrated best-in-class Google Cloud services\n",
    "         - End-to-end data lineage for root cause and impact analysis\n",
    "         - Built with an open source core (CDAP) for pipeline portability\n",
    "\n",
    "\n",
    "- **Google Cloud Data Flow**: Advanced programming skills\n",
    "    - Unified stream and batch data processing that's serverless, fast, and cost-effective.\n",
    "    - New customers get $300 in free credits to spend on Dataflow or other Google Cloud products during the first 90 days.Â \n",
    "\n",
    "\n",
    "- **Google Cloud Looker**: \n",
    "     - Looker is a business intelligence software and big data analytics platform that helps you explore, analyze and share real-time business analytics easily.\n",
    "     \n",
    "- **Google Cloud Data Studio**: Google Cloud: lets us eaily gather and use all your insights -- from CSVs, Analytics, Google Ads, Google heets, BigQuery, and other sources.\n",
    "\n",
    "- **Google Cloud AutoML (Google)**: Trains high-quality custom machine learning models with minimal effort and machine learning expertise.\n",
    "\n",
    "- **Google Cloud Data Catalog**: a fully managed and highlyscalable data discovery and metadata management service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Common Data Pipeline Design Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline Components\n",
    "- Data Pipeline\n",
    "    - Batch\n",
    "    - Streaming\n",
    "    - Lambda Architecture\n",
    "- DAG: Directed Acyclic Graph, upstreamt to downstream\n",
    "- Source: point of data consumption\n",
    "- Sink: point of data consumption/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Design Pattern\n",
    "- A very traditional Pattern\n",
    "<img src = './figs/dpl5.png'>\n",
    "\n",
    "- Traditional DWH Design\n",
    "- DAG can be a combination of sub-DAG\n",
    "- DAGs are orchestrated (Scheduled to do extraction, apply some transformation, and loading)\n",
    "    - Apache Airflow: For scheduling events/orchestrating\n",
    "- Source --> source: we cannot impact the operational system while fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELT Design Pattern\n",
    "<img src = './figs/dpl6.png'>\n",
    "\n",
    "- Good Design to store data in Raw state (Data Lakes)\n",
    "- Helps in CDC Design\n",
    "- Transformation via SQL Queries (Business Logic)\n",
    "- Needs More Computational Power <-- downside\n",
    "- Suited For Cloud --> ELT is mostly used in cloud solution\n",
    "\n",
    "        Cloud SQL --> Cloud Storage ---> BigQuery (has massive processing resources)\n",
    "        \n",
    "- CDC is built on top of ELT Design pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC (Change Data Capture) \n",
    "<img src = './figs/dpl7.png'>\n",
    "\n",
    "- Track changes @ Sources\n",
    "- Get the latest record\n",
    "- Based on ELT Pattern\n",
    "- CDC Tools - Oracle CDC, attunity cdc\n",
    "- Read from DB Logs\n",
    "- Take the record with the maximum version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EtLT Design Pattern\n",
    "- t is mini-transform\n",
    "- Diconnected from business logic\n",
    "- Does mini 't' at early stages of pipeline.\n",
    "- could help in Latency\n",
    "\n",
    "<img src = './figs/dpl8.png'>\n",
    "- Mini-transformation prevents from allowing garbage to go ahead:\n",
    "    - Data Duplication\n",
    "    - Parse URL parameters\n",
    "    - Mask or Hide Sensitive Data (GDPR Compliance)\n",
    "    - No Business Logic isperformed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Client-Server Architecture For Python Objects Transfer\n",
    "- Serialize python objects (list, dictionaty, .mat, dataframe ...) @Tx\n",
    "- Deserialize python objects (list, dictionaty, .mat, dataframe ...) @Rx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Create Server Socket Object\n",
    "serv_socket =  socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Get IP and Port Tuple\n",
    "host = socket.gethostname()\n",
    "port =  55555\n",
    "\n",
    "# Bind the port and IP\n",
    "serv_socket.bind((host, port))\n",
    "\n",
    "# Listen for upcoming requests\n",
    "serv_socket .listen(5)\n",
    "\n",
    "# Set Server in infinite loop\n",
    "while True:\n",
    "    conn, addr = serv_socket.accept()\n",
    "    \n",
    "    msg = 'Hello'\n",
    "    conn.send(msg.econde('utf-8')) # 'ascii'\n",
    "    \n",
    "    conn.recv(RECV_BYTES)\n",
    "    \n",
    "serv_socket.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Create Client Socket Obect\n",
    "client_socket =  socket.socket()\n",
    "\n",
    "# Define The port \n",
    "port =  55555\n",
    "\n",
    "# Connect tot he server on local computer\n",
    "client_socket.connect(('127.0.0.1', port))\n",
    "\n",
    "# Listen for upcoming requests\n",
    "serv_socket .listen(5)\n",
    "\n",
    "# Send Data to Server\n",
    "msg = 'Hi, from client'\n",
    "client_socket.send(msg.econde('utf-8')) # 'ascii'\n",
    "\n",
    "# Receive Data From Server\n",
    "RECV_BYTES = 1024**2\n",
    "client_socket.recv(RECV_BYTES)\n",
    "\n",
    "# Close the connection\n",
    "client_socket.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                       ~END~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
